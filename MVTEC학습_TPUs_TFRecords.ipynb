{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MVTEC학습_TPUs_TFRecords",
      "provenance": [],
      "collapsed_sections": [
        "clSFHJkFNylD"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iskra3138/colab_repo/blob/master/MVTEC%ED%95%99%EC%8A%B5_TPUs_TFRecords.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ovFDeMgtjqW4"
      },
      "source": [
        "# TPUs in Colab&nbsp; <a href=\"https://cloud.google.com/tpu/\"><img valign=\"middle\" src=\"https://raw.githubusercontent.com/GoogleCloudPlatform/tensorflow-without-a-phd/master/tensorflow-rl-pong/images/tpu-hexagon.png\" width=\"50\"></a>\n",
        "In this example, we'll work through training a model to classify images of\n",
        "flowers on Google's lightning-fast Cloud TPUs. Our model will take as input a photo of a flower and return whether it is a daisy, dandelion, rose, sunflower, or tulip.\n",
        "\n",
        "We use the Keras framework, new to TPUs in TF 2.1.0. Adapted from [this notebook](https://colab.research.google.com/github/GoogleCloudPlatform/training-data-analyst/blob/master/courses/fast-and-lean-data-science/07_Keras_Flowers_TPU_xception_fine_tuned_best.ipynb) by [Martin Gorner](https://twitter.com/martin_gorner)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "clSFHJkFNylD"
      },
      "source": [
        "#### License"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hleIN5-pcr0N"
      },
      "source": [
        "Copyright 2019-2020 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This is not an official Google product but sample code provided for an educational purpose.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_pQCOmISAQBu"
      },
      "source": [
        "## Enabling and testing the TPU\n",
        "\n",
        "First, you'll need to enable TPUs for the notebook:\n",
        "\n",
        "- Navigate to Edit→Notebook Settings\n",
        "- select TPU from the Hardware Accelerator drop-down\n",
        "\n",
        "Next, we'll check that we can connect to the TPU:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FpvUOuC3j27n",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "except ValueError:\n",
        "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vFIMfPmgQa0h",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Jhwrs4y0NUAi",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "BUCKET = \"gs://iskra3138_mvtec_tfrecords/\"  #@param {type:\"string\", default:\"jddj\"}\n",
        "assert re.search(r'gs://.+', BUCKET), 'For this part, you need a GCS bucket. Head to http://console.cloud.google.com/storage and create one.'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kvPXiovhi3ZZ"
      },
      "source": [
        "\n",
        "## Input data\n",
        "\n",
        "Our input data is stored on Google Cloud Storage. To more fully use the parallelism TPUs offer us, and to avoid bottlenecking on data transfer, we've stored our input data in TFRecord files, 230 images per file.\n",
        "\n",
        "Below, we make heavy use of `tf.data.experimental.AUTOTUNE` to optimize different parts of input loading.\n",
        "\n",
        "All of these techniques are a bit overkill for our (small) dataset, but demonstrate best practices for using TPUs.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65rbsgnMrZQ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "\n",
        "IMG_WIDTH = 224 #@param {type:\"integer\"}\n",
        "IMG_HEIGHT = 224 #@param {type:\"integer\"}\n",
        "IMAGE_SIZE =  [IMG_HEIGHT, IMG_WIDTH]\n",
        "\n",
        "batch_size = 8 * tpu_strategy.num_replicas_in_sync\n",
        "\n",
        "train_fns = os.path.join(BUCKET,'train.tfrecords')\n",
        "validation_fns = os.path.join(BUCKET,'valid.tfrecords')\n",
        "\n",
        "def parse_tfrecord(example):\n",
        "    features = {\n",
        "        'height': tf.io.FixedLenFeature([], tf.int64),\n",
        "        'width': tf.io.FixedLenFeature([], tf.int64),\n",
        "        'depth': tf.io.FixedLenFeature([], tf.int64),\n",
        "        'label': tf.io.FixedLenFeature([], tf.int64),\n",
        "        'image_raw': tf.io.FixedLenFeature([], tf.string),\n",
        "    }\n",
        "    # decode the TFRecord\n",
        "    example = tf.io.parse_single_example(example, features)\n",
        "    \n",
        "    # FixedLenFeature fields are now ready to use: exmple['size']\n",
        "    # VarLenFeature fields require additional sparse_to_dense decoding\n",
        "    \n",
        "    label = example['label']\n",
        "    label = tf.one_hot(indices=label,\n",
        "                      depth=2\n",
        "                      )\n",
        "    image = tf.io.decode_jpeg(example['image_raw'], channels=3)\n",
        "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "    image = tf.image.resize(image, [IMG_WIDTH, IMG_HEIGHT])\n",
        "    \n",
        "    return image, label\n",
        "\n",
        "def load_dataset(filenames):\n",
        "  # Read from TFRecords. For optimal performance, we interleave reads from multiple files.\n",
        "  records = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO)\n",
        "  return records.map(parse_tfrecord, num_parallel_calls=AUTO)\n",
        "\n",
        "def get_training_dataset():\n",
        "  dataset = load_dataset(train_fns)\n",
        "\n",
        "  # Create some additional training images by randomly flipping and\n",
        "  # increasing/decreasing the saturation of images in the training set. \n",
        "  def data_augment(image, label):\n",
        "    modified = tf.image.random_flip_left_right(image)\n",
        "    modified = tf.image.random_flip_up_down(modified)\n",
        "    return modified, label\n",
        "  augmented = dataset.map(data_augment, num_parallel_calls=AUTO)\n",
        "\n",
        "  # Prefetch the next batch while training (autotune prefetch buffer size).\n",
        "  return augmented.repeat().shuffle(2048).batch(batch_size).prefetch(AUTO) \n",
        "\n",
        "training_dataset = get_training_dataset()\n",
        "validation_dataset = load_dataset(validation_fns).batch(batch_size).prefetch(AUTO)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KTukaIGil7_m"
      },
      "source": [
        "Let's take a peek at the training dataset we've created:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-DwfsJq0l_DJ",
        "colab": {}
      },
      "source": [
        "CLASSES = ['NG', 'OK']\n",
        "\n",
        "def display_one(image, title, subplot, color):\n",
        "  plt.subplot(subplot)\n",
        "  plt.axis('off')\n",
        "  plt.imshow(image)\n",
        "  plt.title(title, fontsize=16, color=color)\n",
        "  \n",
        "# If model is provided, use it to generate predictions.\n",
        "def display_nine(images, titles, title_colors=None):\n",
        "  subplot = 331\n",
        "  plt.figure(figsize=(13,13))\n",
        "  for i in range(9):\n",
        "    color = 'black' if title_colors is None else title_colors[i]\n",
        "    display_one(images[i], titles[i], 331+i, color)\n",
        "  plt.tight_layout()\n",
        "  plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
        "  plt.show()\n",
        "\n",
        "def get_dataset_iterator(dataset, n_examples):\n",
        "  return dataset.unbatch().batch(n_examples).as_numpy_iterator()\n",
        "\n",
        "training_viz_iterator = get_dataset_iterator(training_dataset, 9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHBE4JfIhKoP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Re-run this cell to show a new batch of images\n",
        "images, classes = next(training_viz_iterator)\n",
        "class_idxs = np.argmax(classes, axis=-1) # transform from one-hot array to class number\n",
        "labels = [CLASSES[idx] for idx in class_idxs]\n",
        "display_nine(images*255.0, labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ALtRUlxhw8Vt"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkWZlETH1Kbu",
        "colab_type": "text"
      },
      "source": [
        "모델 compile 시, tf.keras.optimizers api를 쓰면 TPU에서는 학습이 되지 않음\n",
        "- optimizer='adam' <- w/callback : working\n",
        "- optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=1e-4) : working\n",
        "- optimizer=tf.keras.optimizers.Adam(lr=1e-4) : not working"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hJl3vNtJOB-x",
        "colab": {}
      },
      "source": [
        "def create_model():\n",
        "  pretrained_model = tf.keras.applications.ResNet101(weights='imagenet', input_shape=[*IMAGE_SIZE, 3], include_top=False)\n",
        "  pretrained_model.trainable = True\n",
        "  x = pretrained_model.output\n",
        "  x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "  predictions = tf.keras.layers.Dense(2, activation='softmax', name='prediction')(x)\n",
        "  model = tf.keras.Model(inputs=pretrained_model.input, outputs=predictions)\n",
        "  \n",
        "  #optimizer='adam',\n",
        "  \n",
        "  model.compile(\n",
        "    optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=1e-4),\n",
        "    loss = 'categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        "  )\n",
        "  return model\n",
        "\n",
        "with tpu_strategy.scope(): # creating the model in the TPUStrategy scope means we will train the model on the TPU\n",
        "  model = create_model()\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dMfenMQcxAAb"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "N0wDLhZOdjAA"
      },
      "source": [
        "Calculate the number of images in each dataset. Rather than actually load the data to do so (expensive), we rely on hints in the filename. This is used to calculate the number of batches per epoch.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-UO_OaLsx9u",
        "colab_type": "code",
        "outputId": "28c37b98-9291-4d1e-9f8a-5b67a246562a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_samples = sum(1 for _ in tf.data.TFRecordDataset(train_fns))\n",
        "print (train_samples)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "22098\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uEDh43QtBT1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_steps = train_samples // batch_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Zs2tsV8xebhU"
      },
      "source": [
        "Actually train the model. While the first epoch will be quite a bit slower as we must XLA-compile the execution graph and load the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VjneDu60AV6",
        "colab_type": "code",
        "outputId": "e09508a1-ffe1-4e0f-bcb3-ea17c7d81ece",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "EPOCHS=5\n",
        "history = model.fit(training_dataset, validation_data=validation_dataset,\n",
        "                    steps_per_epoch=train_steps, epochs=EPOCHS)\n",
        "#history = model.fit(training_dataset, validation_data=validation_dataset,\n",
        "#                    steps_per_epoch=train_steps, epochs=EPOCHS, callbacks=[lr_callback])\n",
        "final_accuracy = history.history[\"val_accuracy\"][-5:]\n",
        "print(\"FINAL ACCURACY MEAN-5: \", np.mean(final_accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 345 steps\n",
            "Epoch 1/5\n",
            "345/345 [==============================] - 151s 437ms/step - loss: 0.0160 - accuracy: 0.9943 - val_loss: 15.8790 - val_accuracy: 0.4353\n",
            "Epoch 2/5\n",
            "345/345 [==============================] - 52s 152ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 19.3712 - val_accuracy: 0.4353\n",
            "Epoch 3/5\n",
            "345/345 [==============================] - 52s 152ms/step - loss: 3.8070e-05 - accuracy: 1.0000 - val_loss: 22.9056 - val_accuracy: 0.4353\n",
            "Epoch 4/5\n",
            "345/345 [==============================] - 53s 154ms/step - loss: 0.0074 - accuracy: 0.9987 - val_loss: 12.2873 - val_accuracy: 0.4353\n",
            "Epoch 5/5\n",
            "345/345 [==============================] - 53s 153ms/step - loss: 6.0836e-04 - accuracy: 0.9999 - val_loss: 0.4508 - val_accuracy: 0.7719\n",
            "FINAL ACCURACY MEAN-5:  0.5026523\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7Qv8rC4aVOFB",
        "outputId": "3eec2369-6b66-40fd-c591-e1bcb6afa811",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "EPOCHS=5\n",
        "history = model.fit(training_dataset, validation_data=validation_dataset,\n",
        "                    steps_per_epoch=train_steps, epochs=EPOCHS,)\n",
        "final_accuracy = history.history[\"val_accuracy\"][-5:]\n",
        "print(\"FINAL ACCURACY MEAN-5: \", np.mean(final_accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 345 steps\n",
            "Epoch 1/5\n",
            "345/345 [==============================] - 80s 231ms/step - loss: 2.1361e-05 - accuracy: 1.0000 - val_loss: 0.6501 - val_accuracy: 0.8641\n",
            "Epoch 2/5\n",
            "345/345 [==============================] - 52s 151ms/step - loss: 1.2382e-05 - accuracy: 1.0000 - val_loss: 0.0432 - val_accuracy: 0.9936\n",
            "Epoch 3/5\n",
            "345/345 [==============================] - 53s 153ms/step - loss: 8.2224e-06 - accuracy: 1.0000 - val_loss: 0.0451 - val_accuracy: 0.9954\n",
            "Epoch 4/5\n",
            "345/345 [==============================] - 52s 152ms/step - loss: 3.8053e-06 - accuracy: 1.0000 - val_loss: 0.0435 - val_accuracy: 0.9954\n",
            "Epoch 5/5\n",
            "345/345 [==============================] - 54s 155ms/step - loss: 2.8234e-06 - accuracy: 1.0000 - val_loss: 0.0450 - val_accuracy: 0.9954\n",
            "FINAL ACCURACY MEAN-5:  0.9687945\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VngeUBIdyJ1T",
        "colab": {}
      },
      "source": [
        "def display_training_curves(training, validation, title, subplot):\n",
        "  ax = plt.subplot(subplot)\n",
        "  ax.plot(training)\n",
        "  ax.plot(validation)\n",
        "  ax.set_title('model '+ title)\n",
        "  ax.set_ylabel(title)\n",
        "  ax.set_xlabel('epoch')\n",
        "  ax.legend(['training', 'validation'])\n",
        "\n",
        "plt.subplots(figsize=(10,10))\n",
        "plt.tight_layout()\n",
        "display_training_curves(history.history['accuracy'], history.history['val_accuracy'], 'accuracy', 211)\n",
        "display_training_curves(history.history['loss'], history.history['val_loss'], 'loss', 212)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "o19yp4TxqmgI"
      },
      "source": [
        "Accuracy goes up and loss goes down. Looks good!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MKFMWzh0Yxsq"
      },
      "source": [
        "## Predictions\n",
        "\n",
        "Let's look at some our model's predictions next to the original images. We'll show 9 images from the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S3aW9KYw26f7",
        "colab": {}
      },
      "source": [
        "def img_title(label, prediction):\n",
        "  # Both prediction (probabilities) and label (one-hot) are arrays with one item per class.\n",
        "  class_idx = np.argmax(label, axis=-1)\n",
        "  prediction_idx = np.argmax(prediction, axis=-1)\n",
        "  if class_idx == prediction_idx:\n",
        "    return f'{CLASSES[prediction_idx]} [correct]', 'black'\n",
        "  else:\n",
        "    return f'{CLASSES[prediction_idx]} [incorrect, should be {CLASSES[class_idx]}]', 'red'\n",
        "\n",
        "def get_titles(images, labels, model):\n",
        "  predictions = model.predict(images)\n",
        "  titles, colors = [], []\n",
        "  for label, prediction in zip(classes, predictions):\n",
        "    title, color = img_title(label, prediction)\n",
        "    titles.append(title)\n",
        "    colors.append(color)\n",
        "  return titles, colors\n",
        "\n",
        "validation_viz_iterator = get_dataset_iterator(validation_dataset, 9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "71fHvf6sjIgX",
        "colab": {}
      },
      "source": [
        "# Re-run this cell to show a new batch of images\n",
        "images, classes = next(validation_viz_iterator)\n",
        "titles, colors = get_titles(images, classes, model)\n",
        "display_nine(images*255.0, titles, colors)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kIC5KKcJOB_A"
      },
      "source": [
        "## Save and re-loading our trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ivTV2qPoOB_B",
        "colab": {}
      },
      "source": [
        "# We can save our model with:\n",
        "model.save('model.h5')\n",
        "# and reload it with:\n",
        "reloaded_model = tf.keras.models.load_model('model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZYxyOmXBjA5C",
        "colab": {}
      },
      "source": [
        "# Re-run this cell to show a new batch of images\n",
        "images, classes = next(validation_viz_iterator)\n",
        "titles, colors = get_titles(images, classes, reloaded_model)\n",
        "display_nine_flowers(images, titles, colors)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}