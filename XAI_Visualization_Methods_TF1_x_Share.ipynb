{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XAI_Visualization Methods_TF1.x_Share",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "rymHjWwnqiFz",
        "75uaCXrYqiFH",
        "cGDBBMvdqiEl"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iskra3138/colab_repo/blob/master/XAI_Visualization_Methods_TF1_x_Share.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Pfk-sNJrqiDT",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bak0snbI6q-m",
        "colab_type": "text"
      },
      "source": [
        "copy a trained model from GCP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMP_PQR51aTE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gsutil cp gs://iskra3138_mvtec_tfrecords/my_mvtec_tpumodel.h5 ./"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8_zo8FExYAb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_file = 'my_mvtec_tpumodel.h5'\n",
        "activation_layer = 'conv5_block3_out'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "20lHD8E3ooMK",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "AHfADBs8QegC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "from PIL import Image as Image\n",
        "import cv2\n",
        "\n",
        "import glob\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.keras.models import model_from_json\n",
        "from tensorflow.python.keras import backend as K\n",
        "from tensorflow.python.framework import ops\n",
        "\n",
        "\n",
        "import scipy\n",
        "from scipy import ndimage\n",
        "from skimage.measure import label, regionprops\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "O1Dq5iO2Qegk",
        "colab_type": "text"
      },
      "source": [
        "# import model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "FwM_8xznQehK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.models.load_model(model_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGBJ9PfRovFT",
        "colab_type": "text"
      },
      "source": [
        "# Make Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4nArxoT3rqc",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title TFRecord Parsing Functions [Run Me!!!!]\n",
        "import os\n",
        "\n",
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "IMAGE_SIZE =  [224, 224]\n",
        "\n",
        "validation_fns = 'gs://iskra3138_mvtec_tfrecords/valid.tfrecords'\n",
        "\n",
        "def parse_tfrecord(example):\n",
        "    features = {\n",
        "        'height': tf.io.FixedLenFeature([], tf.int64),\n",
        "        'width': tf.io.FixedLenFeature([], tf.int64),\n",
        "        'depth': tf.io.FixedLenFeature([], tf.int64),\n",
        "        'label': tf.io.FixedLenFeature([], tf.int64),\n",
        "        'image_raw': tf.io.FixedLenFeature([], tf.string),\n",
        "    }\n",
        "    # decode the TFRecord\n",
        "    example = tf.io.parse_single_example(example, features)\n",
        "    \n",
        "    label = example['label']\n",
        "    #label = tf.one_hot(indices=label, depth=2)\n",
        "    image = tf.io.decode_jpeg(example['image_raw'], channels=3)\n",
        "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "    image = tf.image.resize(image, IMAGE_SIZE)\n",
        "    \n",
        "    return image, label\n",
        "\n",
        "def load_dataset(filenames):\n",
        "  # Read from TFRecords. For optimal performance, we interleave reads from multiple files.\n",
        "  records = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO)\n",
        "  return records.map(parse_tfrecord, num_parallel_calls=AUTO)\n",
        "\n",
        "def NG_filter_fn(img, label):\n",
        "  return tf.math.equal(label, 0)\n",
        "def OK_filter_fn(img, label):\n",
        "  return tf.math.equal(label, 1)\n",
        "\n",
        "def make_class_sapling (fns, class_idx, batch_size) :\n",
        "  if class_idx == 0 :\n",
        "    NG_dataset = load_dataset(fns).filter(NG_filter_fn).shuffle(1000).batch(batch_size).prefetch(AUTO).repeat()\n",
        "    NG_iter = NG_dataset.make_one_shot_iterator()\n",
        "    return NG_iter\n",
        "  else :\n",
        "    OK_dataset = load_dataset(fns).filter(OK_filter_fn).shuffle(1000).batch(batch_size).prefetch(AUTO).repeat()\n",
        "    OK_iter = OK_dataset.make_one_shot_iterator()\n",
        "    return OK_iter\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJZjt4aEpuhD",
        "colab_type": "text"
      },
      "source": [
        "# Import GradCAM & Guided GradCAM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jC3e6Cw3toiY",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title import Util Code [Run Me!!]\n",
        "def deprocess_image(x):\n",
        "    '''\n",
        "    Same normalization as in:\n",
        "    https://github.com/fchollet/keras/blob/master/examples/conv_filter_visualization.py\n",
        "    '''\n",
        "    if np.ndim(x) > 3:\n",
        "        x = np.squeeze(x)\n",
        "    # normalize tensor: center on 0., ensure std is 0.1\n",
        "    x -= x.mean()\n",
        "    x /= (x.std() + 1e-5)\n",
        "    x *= 0.1\n",
        "\n",
        "    # clip to [0, 1]\n",
        "    x += 0.5\n",
        "    x = np.clip(x, 0, 1)\n",
        "\n",
        "    # convert to RGB array\n",
        "    x *= 255\n",
        "    if tf.keras.backend.image_data_format() == 'channels_first':\n",
        "        x = x.transpose((1, 2, 0))\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x\n",
        "\n",
        "def generate_bbox(img, cam, threshold):\n",
        "    labeled, nr_objects = ndimage.label(cam > threshold)\n",
        "    props = regionprops(labeled)\n",
        "    return props"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6h_j1kakUjiB",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title import Grad-CAM Code [Run Me!!]\n",
        "class GradCAM:\n",
        "  def __init__(self, model, activation_layer):\n",
        "    self.model = model\n",
        "    self.activation_layer = activation_layer\n",
        "    self.gradient_fn = self._get_gradcam_tensor_function()\n",
        "\n",
        "  # get partial tensor graph of CNN model\n",
        "  def _get_gradcam_tensor_function(self):\n",
        "    model_input = self.model.input\n",
        "    class_idx = tf.argmax(model.output[0])\n",
        "    \n",
        "    y_c = self.model.outputs[0].op.inputs[0][0, class_idx]\n",
        "    A_k = self.model.get_layer(self.activation_layer).output\n",
        "    \n",
        "    grads = K.gradients(y_c, A_k)[0]\n",
        "    gradient_fn = K.function([model.input], [A_k, grads, model.output])\n",
        "    return gradient_fn\n",
        "\n",
        "  # generate Grad-CAM\n",
        "  def generate(self, input_tensor):\n",
        "    conv_output, grad_val, predictions = self.gradient_fn([input_tensor])\n",
        "    conv_output, grad_val = conv_output[0], grad_val[0]\n",
        "    \n",
        "    weights = np.mean(grad_val, axis=(0, 1))\n",
        "    gradcam = np.dot(conv_output, weights)\n",
        "    \n",
        "    gradcam = cv2.resize(gradcam, (224, 224))\n",
        "    \n",
        "    ## Relu\n",
        "    gradcam = np.maximum(gradcam, 0)\n",
        "    return gradcam, predictions[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ccKK_ditGAY",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title import Guided Grad-CAM Code [Run Me!!]\n",
        "\n",
        "class GuidedGradCam:\n",
        "    def __init__(self, model, activation_layer, method='GuidedBackProp'):\n",
        "        self.model = model\n",
        "        #self.model_func = model_func\n",
        "        self.activation_layer = activation_layer\n",
        "\n",
        "        if method == 'BackProp':\n",
        "            self._register_backprop_gradient()\n",
        "            self.guided_model = self._modify_graph('BackProp')\n",
        "        elif method == 'DeconvNet':\n",
        "            self._register_deconvnet_gradient()\n",
        "            self.guided_model = self._modify_graph('DeconvNet')\n",
        "        elif method == 'GuidedBackProp':\n",
        "            self._register_guidedbackprop_gradient()\n",
        "            self.guided_model = self._modify_graph('GuidedBackProp')\n",
        "        else:\n",
        "            sys.exit('method must be (BackProp, DeconvNet, GuidedBackProp)')\n",
        "\n",
        "        self.tensor_function = self.get_tensor_function()\n",
        "\n",
        "    # register gradient\n",
        "    def _register_backprop_gradient(self):\n",
        "        if \"BackProp\" not in ops._gradient_registry._registry:\n",
        "            @ops.RegisterGradient(\"BackProp\")\n",
        "            def _BackProp(op, grad):\n",
        "                dtype = op.inputs[0].dtype\n",
        "                return grad * tf.cast(op.inputs[0] > 0., dtype)\n",
        "\n",
        "    def _register_deconvnet_gradient(self):\n",
        "        if \"DeconvNet\" not in ops._gradient_registry._registry:\n",
        "            @ops.RegisterGradient(\"DeconvNet\")\n",
        "            def _DeconvNet(op, grad):\n",
        "                dtype = op.inputs[0].dtype\n",
        "                return grad * tf.cast(grad > 0., dtype)\n",
        "\n",
        "    def _register_guidedbackprop_gradient(self):\n",
        "        if \"GuidedBackProp\" not in ops._gradient_registry._registry:\n",
        "            @ops.RegisterGradient(\"GuidedBackProp\")\n",
        "            def _GuidedBackProp(op, grad):\n",
        "                dtype = op.inputs[0].dtype\n",
        "                return grad * tf.cast(grad > 0., dtype) * \\\n",
        "                       tf.cast(op.inputs[0] > 0., dtype)\n",
        "\n",
        "    # modify model graph\n",
        "    def _modify_graph(self, name):\n",
        "        g = tf.get_default_graph()\n",
        "        with g.gradient_override_map({'Relu': name}):\n",
        "            new_model = tf.keras.models.load_model('my_mvtec_tpumodel.h5')\n",
        "        return new_model\n",
        "\n",
        "    # get partial tensor graph of CNN model\n",
        "    def get_tensor_function(self, method='max', channel=0):\n",
        "        model_input = self.guided_model.input\n",
        "        layer_output = self.guided_model.get_layer(self.activation_layer).output\n",
        "\n",
        "        '''if method == 'max':\n",
        "            output = K.max(layer_output, axis=3)\n",
        "        elif method == 'one':\n",
        "            output = layer_output[:, :, :, channel]\n",
        "        else:\n",
        "            sys.exit('method must be (max, one)')'''\n",
        "\n",
        "        tensor_function = K.function([model_input], [K.gradients(layer_output, model_input)[0]])\n",
        "        return tensor_function\n",
        "\n",
        "    # generate saliency map(gradient)\n",
        "    def generate(self, input_tensor, gradcam):\n",
        "        saliency = self.tensor_function([input_tensor])[0]\n",
        "        guided_grad_cam = saliency * gradcam[...,np.newaxis]\n",
        "\n",
        "        return guided_grad_cam[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9r8ZiKBNOgTB",
        "colab_type": "text"
      },
      "source": [
        "### gradcam, guided_gradcma class 객체 선언"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkbCL75CNEdO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gradcam_gen = GradCAM(model, activation_layer)\n",
        "guided_gradcam_gen = GuidedGradCam(model, activation_layer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpIpVbGR2blo",
        "colab_type": "text"
      },
      "source": [
        "### Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E84JvcPw74v7",
        "colab_type": "text"
      },
      "source": [
        "Make Batch Data randomly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRBAJV3G8C-I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_size = 5 # viualization할 이미지 개수\n",
        "class_idx = 0 # NG 이면 0, OK 이면 1\n",
        "\n",
        "iter = make_class_sapling(validation_fns, class_idx = class_idx, batch_size = sample_size)\n",
        "images, labels = iter.get_next()\n",
        "with tf.Session() as sess:\n",
        "  imgs = sess.run(images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YODLDNbhPvf8",
        "colab_type": "text"
      },
      "source": [
        "Make gradcam, guided_gradcam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpQ_AD1xzArj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gradcams=[]\n",
        "guided_gradcams=[]\n",
        "\n",
        "for i, img in enumerate(imgs):\n",
        "    \n",
        "    img_tensor = np.expand_dims(img, axis=0)\n",
        "    \n",
        "    gradcam, pred = gradcam_gen.generate(img_tensor)\n",
        "    gradcams.append(gradcam)\n",
        "    \n",
        "    guided_gradcam = guided_gradcam_gen.generate(img_tensor, gradcam)\n",
        "    guided_gradcams.append(guided_gradcam)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZjHtKFO1Zjl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, axes = plt.subplots(sample_size, 4, figsize=(10, 10))\n",
        "for i, img in enumerate(imgs):\n",
        "\n",
        "    img =img\n",
        "    props = generate_bbox(img, gradcams[i], 0.5) # (원본이미지, gradcam값, BBOX 작성을 위한 threshold)\n",
        "    \n",
        "    axes[i, 0].imshow(img)\n",
        "    axes[i, 1].imshow(img)\n",
        "    axes[i, 1].imshow(gradcams[i], cmap='jet', alpha=0.5)\n",
        "    axes[i, 2].imshow(img)\n",
        "    for b in props:\n",
        "        bbox = b.bbox\n",
        "        xs = bbox[1]\n",
        "        ys = bbox[0]\n",
        "        w = bbox[3] - bbox[1]\n",
        "        h = bbox[2] - bbox[0]\n",
        "\n",
        "        rect = patches.Rectangle((xs, ys), w, h, linewidth=2, edgecolor='r', facecolor='none')\n",
        "        axes[i, 2].add_patch(rect)\n",
        "        \n",
        "    axes[i, 3].imshow(deprocess_image(guided_gradcams[i]))\n",
        "    \n",
        "    \n",
        "    axes[i, 0].axis('off')\n",
        "    axes[i, 1].axis('off')\n",
        "    axes[i, 2].axis('off')\n",
        "    axes[i, 3].axis('off')\n",
        "    \n",
        "    axes[0, 0].set_title(\"image\", fontsize=18)\n",
        "    axes[0, 1].set_title(\"Grad-CAM\", fontsize=18)\n",
        "    axes[0, 2].set_title(\"BBOX\", fontsize=18)\n",
        "    axes[0, 3].set_title(\"Guided-Grad-CAM\", fontsize=18)\n",
        "    \n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rymHjWwnqiFz"
      },
      "source": [
        "# IntegratedGradients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OcgedaozqiF0"
      },
      "source": [
        "출처 ; https://github.com/hiranumn/IntegratedGradients/blob/master/examples/VGG%20example.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "cellView": "form",
        "id": "T8xxWBYyqiF0",
        "colab": {}
      },
      "source": [
        "#@title IntegratedGradients Class import [Run Me!!!!]\n",
        "################################################################\n",
        "# Implemented by Naozumi Hiranuma (hiranumn@uw.edu)            #\n",
        "#                                                              #\n",
        "# Keras-compatible implmentation of Integrated Gradients       # \n",
        "# proposed in \"Axiomatic attribution for deep neuron networks\" #\n",
        "# (https://arxiv.org/abs/1703.01365).                          #\n",
        "#                                                              #\n",
        "# Keywords: Shapley values, interpretable machine learning     #\n",
        "################################################################\n",
        "\n",
        "from __future__ import division, print_function\n",
        "import numpy as np\n",
        "from time import sleep\n",
        "import sys\n",
        "import keras.backend as K\n",
        "\n",
        "from keras.models import Model, Sequential\n",
        "\n",
        "'''\n",
        "Integrated gradients approximates Shapley values by integrating partial\n",
        "gradients with respect to input features from reference input to the\n",
        "actual input. The following class implements the paper \"Axiomatic attribution\n",
        "for deep neuron networks\".\n",
        "'''\n",
        "class integrated_gradients:\n",
        "    # model: Keras model that you wish to explain.\n",
        "    # outchannels: In case the model are multi tasking, you can specify which output you want explain .\n",
        "    def __init__(self, model, outchannels=[], verbose=1):\n",
        "    \n",
        "        #get backend info (either tensorflow or theano)\n",
        "        self.backend = K.backend()\n",
        "        \n",
        "        #load model supports keras.Model and keras.Sequential\n",
        "        if isinstance(model, Sequential) or isinstance(model, tf.keras.Sequential)  :\n",
        "            self.model = model.model\n",
        "            \n",
        "        elif isinstance(model, Model) or isinstance(model, tf.keras.Model):\n",
        "            self.model = model\n",
        "            \n",
        "        else:\n",
        "            print(\"Invalid input model\")\n",
        "            #return -1\n",
        "        \n",
        "        #load input tensors\n",
        "        self.input_tensors = []\n",
        "        for i in self.model.inputs:\n",
        "            self.input_tensors.append(i)\n",
        "        # The learning phase flag is a bool tensor (0 = test, 1 = train)\n",
        "        # to be passed as input to any Keras function that uses \n",
        "        # a different behavior at train time and test time.\n",
        "        self.input_tensors.append(K.learning_phase())\n",
        "        \n",
        "        #If outputchanels are specified, use it.\n",
        "        #Otherwise evalueate all outputs.\n",
        "        self.outchannels = outchannels\n",
        "        if len(self.outchannels) == 0: \n",
        "            if verbose: print(\"Evaluated output channel (0-based index): All\")\n",
        "            if K.backend() == \"tensorflow\":\n",
        "                self.outchannels = range(self.model.output.shape[1]._value)\n",
        "            elif K.backend() == \"theano\":\n",
        "                self.outchannels = range(self.model.output._keras_shape[1])\n",
        "        else:\n",
        "            if verbose: \n",
        "                print(\"Evaluated output channels (0-based index):\")\n",
        "                print(','.join([str(i) for i in self.outchannels]))\n",
        "                \n",
        "        #Build gradient functions for desired output channels.\n",
        "        self.get_gradients = {}\n",
        "        if verbose: print(\"Building gradient functions\")\n",
        "        \n",
        "        # Evaluate over all requested channels.\n",
        "        for c in self.outchannels:\n",
        "            # Get tensor that calculates gradient\n",
        "            #gradients = self.model.optimizer.get_gradients(self.model.output[:, c], self.model.input)\n",
        "            gradients = tf.gradients(self.model.output[:, c], self.model.input)[0]\n",
        "\n",
        "            # Build computational graph that computes the tensors given inputs\n",
        "            self.get_gradients[c] = tf.keras.backend.function([self.input_tensors], [gradients])\n",
        "            \n",
        "            # This takes a lot of time for a big model with many tasks.\n",
        "            # So lets print the progress.\n",
        "            if verbose:\n",
        "                sys.stdout.write('\\r')\n",
        "                sys.stdout.write(\"Progress: \"+str(int((c+1)*1.0/len(self.outchannels)*1000)*1.0/10)+\"%\")\n",
        "                sys.stdout.flush()\n",
        "        # Done\n",
        "        if verbose: print(\"\\nDone.\")\n",
        "            \n",
        "                \n",
        "    '''\n",
        "    Input: sample to explain, channel to explain\n",
        "    Optional inputs:\n",
        "        - reference: reference values (defaulted to 0s).\n",
        "        - steps: # steps from reference values to the actual sample (defualted to 50).\n",
        "    Output: list of numpy arrays to integrated over.\n",
        "    '''\n",
        "    def explain(self, sample, outc=0, reference=False, num_steps=50, verbose=0):\n",
        "        \n",
        "        # Each element for each input stream.\n",
        "        samples = []\n",
        "        numsteps = []\n",
        "        step_sizes = []\n",
        "        \n",
        "        # If multiple inputs are present, feed them as list of np arrays. \n",
        "        if isinstance(sample, list):\n",
        "            #If reference is present, reference and sample size need to be equal.\n",
        "            if reference != False: \n",
        "                assert len(sample) == len(reference)\n",
        "            for i in range(len(sample)):\n",
        "                if reference == False:\n",
        "                    _output = integrated_gradients.linearly_interpolate(sample[i], False, num_steps)\n",
        "                else:\n",
        "                    _output = integrated_gradients.linearly_interpolate(sample[i], reference[i], num_steps)\n",
        "                samples.append(_output[0])\n",
        "                numsteps.append(_output[1])\n",
        "                step_sizes.append(_output[2])\n",
        "        \n",
        "        # Or you can feed just a single numpy arrray. \n",
        "        elif isinstance(sample, np.ndarray):\n",
        "            _output = integrated_gradients.linearly_interpolate(sample, reference, num_steps)\n",
        "            samples.append(_output[0])\n",
        "            numsteps.append(_output[1])\n",
        "            step_sizes.append(_output[2])\n",
        "            \n",
        "        # Desired channel must be in the list of outputchannels\n",
        "        assert outc in self.outchannels\n",
        "        if verbose: print(\"Explaning the \"+str(self.outchannels[outc])+\"th output.\")\n",
        "            \n",
        "        # For tensorflow backend\n",
        "        _input = []\n",
        "        for s in samples:\n",
        "            _input.append(s)\n",
        "        _input.append(0)\n",
        "        \n",
        "        gradients = self.get_gradients[outc](_input)\n",
        "        \n",
        "        \n",
        "        explanation = []\n",
        "        for i in range(len(gradients)):\n",
        "            _temp = np.sum(gradients[i], axis=0)\n",
        "            explanation.append(np.multiply(_temp, step_sizes[i]))\n",
        "           \n",
        "        # Format the return values according to the input sample.\n",
        "        if isinstance(sample, list):\n",
        "            return explanation\n",
        "        elif isinstance(sample, np.ndarray):\n",
        "            return explanation[0]\n",
        "        return -1\n",
        "\n",
        "    \n",
        "    '''\n",
        "    Input: numpy array of a sample\n",
        "    Optional inputs:\n",
        "        - reference: reference values (defaulted to 0s).\n",
        "        - steps: # steps from reference values to the actual sample.\n",
        "    Output: list of numpy arrays to integrate over.\n",
        "    '''\n",
        "    @staticmethod\n",
        "    def linearly_interpolate(sample, reference=False, num_steps=50):\n",
        "        # Use default reference values if reference is not specified\n",
        "        if reference is False: reference = np.zeros(sample.shape);\n",
        "\n",
        "        # Reference and sample shape needs to match exactly\n",
        "        assert sample.shape == reference.shape\n",
        "\n",
        "        # Calcuated stepwise difference from reference to the actual sample.\n",
        "        ret = np.zeros(tuple([num_steps] +[i for i in sample.shape]))\n",
        "        for s in range(num_steps):\n",
        "            ret[s] = reference+(sample-reference)*(s*1.0/num_steps)\n",
        "\n",
        "        return ret, num_steps, (sample-reference)*(1.0/num_steps)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtxIHQBH9roF",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title VisualizationLibrary.visualization_lib [Run Me!!!!]\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "\n",
        "from io import StringIO\n",
        "from io import BytesIO\n",
        "from IPython.display import display\n",
        "from IPython.display import Image\n",
        "from scipy import ndimage\n",
        "\n",
        "\n",
        "def ConvertToGrayscale(attributions):\n",
        "  return np.average(attributions, axis=2)\n",
        "\n",
        "\n",
        "def Polarity(attributions, polarity):\n",
        "  if polarity == 'positive':\n",
        "    return np.clip(attributions, 0, 1)\n",
        "  elif polarity == 'negative':\n",
        "    return np.clip(attributions, -1, 0)\n",
        "  else:\n",
        "    raise ValueError('Unrecognized polarity option.')\n",
        "\n",
        "\n",
        "def ComputeThresholdByTopPercentage(attributions,\n",
        "                                    percentage=60,\n",
        "                                    plot_distribution=True):\n",
        "  \"\"\"Compute the threshold value that maps to the top percentage of values.\n",
        "\n",
        "  This function takes the cumulative sum of attributions and computes the set\n",
        "  of top attributions that contribute to the given percentage of the total sum.\n",
        "  The lowest value of this given set is returned.\n",
        "\n",
        "  Args:\n",
        "    attributions: (numpy.array) The provided attributions.\n",
        "    percentage: (float) Specified percentage by which to threshold.\n",
        "    plot_distribution: (bool) If true, plots the distribution of attributions\n",
        "      and indicates the threshold point by a vertical line.\n",
        "\n",
        "  Returns:\n",
        "    (float) The threshold value.\n",
        "\n",
        "  Raises:\n",
        "    ValueError: if percentage is not in [0, 100].\n",
        "  \"\"\"\n",
        "  if percentage < 0 or percentage > 100:\n",
        "    raise ValueError('percentage must be in [0, 100]')\n",
        "  \n",
        "  # For percentage equal to 100, this should in theory return the lowest\n",
        "  # value as the threshold. However, due to precision errors in numpy's cumsum,\n",
        "  # the last value won't sum to 100%. Thus, in this special case, we force the\n",
        "  # threshold to equal the min value.\n",
        "  if percentage == 100:\n",
        "    return np.min(attributions)\n",
        "\n",
        "  flat_attributions = attributions.flatten()\n",
        "  attribution_sum = np.sum(flat_attributions)\n",
        "  \n",
        "  # Sort the attributions from largest to smallest.\n",
        "  sorted_attributions = np.sort(np.abs(flat_attributions))[::-1]\n",
        "\n",
        "  # Compute a normalized cumulative sum, so that each attribution is mapped to\n",
        "  # the percentage of the total sum that it and all values above it contribute.\n",
        "  cum_sum = 100.0 * np.cumsum(sorted_attributions) / attribution_sum\n",
        "  threshold_idx = np.where(cum_sum >= percentage)[0][0]\n",
        "  threshold = sorted_attributions[threshold_idx]\n",
        "\n",
        "  if plot_distribution:\n",
        "    # Generate a plot of sorted intgrad scores.\n",
        "    values_to_plot = np.where(cum_sum >= 95)[0][0]\n",
        "    values_to_plot = max(values_to_plot, threshold_idx)\n",
        "    plt.plot(np.arange(values_to_plot), sorted_attributions[:values_to_plot])\n",
        "    plt.axvline(x=threshold_idx)\n",
        "    plt.show()\n",
        "\n",
        "  return threshold\n",
        "    \n",
        "\n",
        "def LinearTransform(attributions,\n",
        "                    clip_above_percentile=99.9,\n",
        "                    clip_below_percentile=70.0,\n",
        "                    low=0.2,\n",
        "                    plot_distribution=False):\n",
        "  \"\"\"Transform the attributions by a linear function.\n",
        "\n",
        "  Transform the attributions so that the specified percentage of top attribution\n",
        "  values are mapped to a linear space between `low` and 1.0.\n",
        "\n",
        "  Args:\n",
        "    attributions: (numpy.array) The provided attributions.\n",
        "    percentage: (float) The percentage of top attribution values.\n",
        "    low: (float) The low end of the linear space.\n",
        "\n",
        "  Returns:\n",
        "    (numpy.array) The linearly transformed attributions.\n",
        "\n",
        "  Raises:\n",
        "    ValueError: if percentage is not in [0, 100].\n",
        "  \"\"\"\n",
        "  if clip_above_percentile < 0 or clip_above_percentile > 100:\n",
        "    raise ValueError('clip_above_percentile must be in [0, 100]')\n",
        "    \n",
        "  if clip_below_percentile < 0 or clip_below_percentile > 100:\n",
        "    raise ValueError('clip_below_percentile must be in [0, 100]')\n",
        "\n",
        "  if low < 0 or low > 1:\n",
        "    raise ValueError('low must be in [0, 1]')\n",
        "\n",
        "  m = ComputeThresholdByTopPercentage(attributions,\n",
        "                                      percentage=100-clip_above_percentile,\n",
        "                                      plot_distribution=plot_distribution)\n",
        "  e = ComputeThresholdByTopPercentage(attributions,\n",
        "                                      percentage=100-clip_below_percentile,\n",
        "                                      plot_distribution=plot_distribution)\n",
        "\n",
        "  # Transform the attributions by a linear function f(x) = a*x + b such that\n",
        "  # f(m) = 1.0 and f(e) = low. Derivation:\n",
        "  #   a*m + b = 1, a*e + b = low  ==>  a = (1 - low) / (m - e)\n",
        "  #                               ==>  b = low - (1 - low) * e / (m - e)\n",
        "  #                               ==>  f(x) = (1 - low) (x - e) / (m - e) + low\n",
        "  transformed = (1 - low) * (np.abs(attributions) - e) / (m - e) + low\n",
        "\n",
        "  # Recover the original sign of the attributions.\n",
        "  transformed *= np.sign(attributions)\n",
        "\n",
        "  # Map values below low to 0.\n",
        "  transformed *= (transformed >= low)\n",
        "  \n",
        "  # Clip values above and below.\n",
        "  transformed = np.clip(transformed, 0.0, 1.0)\n",
        "  return transformed\n",
        "\n",
        "\n",
        "def Binarize(attributions, threshold=0.001):\n",
        "  return attributions > threshold\n",
        "\n",
        "\n",
        "def MorphologicalCleanup(attributions, structure=np.ones((4,4))):\n",
        "  closed = ndimage.grey_closing(attributions, structure=structure)\n",
        "  opened = ndimage.grey_opening(closed, structure=structure)\n",
        "  \n",
        "  return opened\n",
        "\n",
        "\n",
        "def Outlines(attributions, percentage=90,\n",
        "             connected_component_structure=np.ones((3,3)),\n",
        "             plot_distribution=True):\n",
        "  # Binarize the attributions mask if not already.\n",
        "  attributions = Binarize(attributions)\n",
        "\n",
        "  attributions = ndimage.binary_fill_holes(attributions)\n",
        "  \n",
        "  # Compute connected components of the transformed mask.\n",
        "  connected_components, num_cc = ndimage.measurements.label(\n",
        "      attributions, structure=connected_component_structure)\n",
        "\n",
        "  # Go through each connected component and sum up the attributions of that\n",
        "  # component.\n",
        "  overall_sum = np.sum(attributions[connected_components > 0])\n",
        "  component_sums = []\n",
        "  for cc_idx in range(1, num_cc + 1):\n",
        "    cc_mask = connected_components == cc_idx\n",
        "    component_sum = np.sum(attributions[cc_mask])\n",
        "    component_sums.append((component_sum, cc_mask))\n",
        "\n",
        "  # Compute the percentage of top components to keep.\n",
        "  sorted_sums_and_masks = sorted(\n",
        "      component_sums, key=lambda x: x[0], reverse=True)\n",
        "  sorted_sums = list(zip(*sorted_sums_and_masks))[0]\n",
        "  cumulative_sorted_sums = np.cumsum(sorted_sums)\n",
        "  cutoff_threshold = percentage * overall_sum / 100\n",
        "  cutoff_idx = np.where(cumulative_sorted_sums >= cutoff_threshold)[0][0]\n",
        "\n",
        "  if cutoff_idx > 2:\n",
        "    cutoff_idx = 2\n",
        "  \n",
        "  # Turn on the kept components.\n",
        "  border_mask = np.zeros_like(attributions)\n",
        "  for i in range(cutoff_idx + 1):\n",
        "    border_mask[sorted_sums_and_masks[i][1]] = 1\n",
        "\n",
        "  if plot_distribution:\n",
        "    plt.plot(np.arange(len(sorted_sums)), sorted_sums)\n",
        "    plt.axvline(x=cutoff_idx)\n",
        "    plt.show()\n",
        "\n",
        "  # Hollow out the mask so that only the border is showing.\n",
        "  eroded_mask = ndimage.binary_erosion(border_mask, iterations=1)\n",
        "  border_mask[eroded_mask] = 0\n",
        "  \n",
        "  return border_mask\n",
        "\n",
        "\n",
        "def Overlay(attributions, image):\n",
        "  return np.clip(0.7 * image + 0.5 * attributions, 0, 255)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def pil_image(x):\n",
        "  \"\"\"Returns a PIL image created from the provided RGB array.\n",
        "\n",
        "  Args:\n",
        "    x: (numpy.array) RGB array of shape [height, width, 3] consisting of values\n",
        "      in range 0-255.\n",
        "\n",
        "  Returns:\n",
        "    The PIL image.\n",
        "  \"\"\"\n",
        "  x = np.uint8(x)\n",
        "  return PIL.Image.fromarray(x)\n",
        "\n",
        "\n",
        "def show_pil_image(pil_img):\n",
        "  \"\"\"Display the provided PIL image.\n",
        "\n",
        "  Args:\n",
        "    pil_img: (PIL.Image) The provided PIL image.\n",
        "  \"\"\"\n",
        "  #f = StringIO()\n",
        "  f = BytesIO()\n",
        "  pil_img.save(f, 'png')\n",
        "  display(Image(data=f.getvalue()))\n",
        "\n",
        "\n",
        "G = [0, 255, 0]\n",
        "R = [255, 0, 0]\n",
        "def Visualize(attributions,\n",
        "              image,\n",
        "              positive_channel=G,\n",
        "              negative_channel=R,\n",
        "              polarity='positive',\n",
        "              clip_above_percentile=99.9,\n",
        "              clip_below_percentile=0,\n",
        "              morphological_cleanup=False,\n",
        "              structure=np.ones((3,3)),\n",
        "              outlines=False,\n",
        "              outlines_component_percentage=90,\n",
        "              overlay=True,\n",
        "              plot_distribution=False):\n",
        "  \n",
        "  if polarity == 'both':\n",
        "    pos_attributions = Visualize(\n",
        "        attributions, image, positive_channel=positive_channel,\n",
        "        negative_channel=negative_channel, polarity='positive',\n",
        "        clip_above_percentile=clip_above_percentile, clip_below_percentile=clip_below_percentile,\n",
        "        morphological_cleanup=morphological_cleanup, outlines=outlines,\n",
        "        outlines_component_percentage=outlines_component_percentage,\n",
        "        overlay=False,\n",
        "        plot_distribution=plot_distribution)\n",
        "    \n",
        "    neg_attributions = Visualize(\n",
        "        attributions, image, positive_channel=positive_channel,\n",
        "        negative_channel=negative_channel, polarity='negative',\n",
        "        clip_above_percentile=clip_above_percentile, clip_below_percentile=clip_below_percentile,\n",
        "        morphological_cleanup=morphological_cleanup, outlines=outlines,\n",
        "        outlines_component_percentage=outlines_component_percentage,\n",
        "        overlay=False,\n",
        "        plot_distribution=plot_distribution)\n",
        "    \n",
        "    attributions = pos_attributions + neg_attributions\n",
        "    \n",
        "    if overlay:\n",
        "      attributions = Overlay(attributions, image)\n",
        "    \n",
        "    return attributions\n",
        "  elif polarity == 'positive':\n",
        "    attributions = Polarity(attributions, polarity=polarity)\n",
        "    channel = positive_channel\n",
        "  elif polarity == 'negative':\n",
        "    attributions = Polarity(attributions, polarity=polarity)\n",
        "    attributions = np.abs(attributions)\n",
        "    channel = negative_channel\n",
        "\n",
        "  attributions = ConvertToGrayscale(attributions)\n",
        "  \n",
        "  attributions = LinearTransform(attributions,\n",
        "                                 clip_above_percentile, clip_below_percentile,\n",
        "                                 0.0,\n",
        "                                 plot_distribution=plot_distribution)\n",
        "  \n",
        "  if morphological_cleanup:\n",
        "    attributions = MorphologicalCleanup(attributions, structure=structure)\n",
        "  if outlines:\n",
        "    attributions = Outlines(attributions,\n",
        "                            percentage=outlines_component_percentage,\n",
        "                            plot_distribution=plot_distribution)\n",
        "  \n",
        "  # Convert to RGB space\n",
        "  attributions = np.expand_dims(attributions, 2) * channel\n",
        "  \n",
        "  if overlay:\n",
        "    attributions = Overlay(attributions, image)\n",
        "\n",
        "  return attributions\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jSWlj3lVqiF2",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DrxM3vqVqiGH",
        "colab": {}
      },
      "source": [
        "labels = ['NG','OK']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VR3eqjxtqiF9"
      },
      "source": [
        "#### Step 1. Load the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QNHpvHhAqiF-",
        "colab": {}
      },
      "source": [
        "model = tf.keras.models.load_model(model_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5zsznpZgqiF_"
      },
      "source": [
        "#### Step 2. Be sure to complie it and add an optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hY96wpaIqiF_",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='sgd', loss='categorical_crossentropy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "N68KkDWmqiGC"
      },
      "source": [
        "#### Step 3. Wrap it with integrated gradients."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9-kPPZ8NqiGC",
        "colab": {}
      },
      "source": [
        "ig = integrated_gradients(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "X8WgIvDFqiF8"
      },
      "source": [
        "### Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "koVT1qf0qiGE"
      },
      "source": [
        "Step 1. Obtain a sample to explain."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3CQWe1zWJtF1",
        "colab": {}
      },
      "source": [
        "sample_size = 1 # viualization할 이미지 개수, 현재 코드 상에서는 1\n",
        "class_idx = 0 # NG 이면 0, OK 이면 1\n",
        "iter = make_class_sapling(validation_fns, class_idx = class_idx , batch_size = sample_size)\n",
        "\n",
        "images, labels = iter.get_next()\n",
        "with tf.Session() as sess:\n",
        "  images = sess.run(images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Qvs6Dt1mqiGE",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(2,2))\n",
        "plt.imshow(images[0])\n",
        "plt.xticks([], [])\n",
        "plt.yticks([], [])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mNGKr9I5qiGF",
        "colab": {}
      },
      "source": [
        "# preprocess image\n",
        "x = images\n",
        "\n",
        "# preprocess reference as well\n",
        "ref = np.zeros((224, 224, 3))\n",
        "ref = np.expand_dims(ref, axis=0)\n",
        "#ref = preprocess_input(ref)\n",
        "ref = ref/255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oQCODwGLqiGH"
      },
      "source": [
        "Step 2. Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8XAws5B-qiGI",
        "colab": {}
      },
      "source": [
        "pred = model.predict(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eVARPGa_qiGJ",
        "colab": {}
      },
      "source": [
        "predicted = np.argmax(pred)\n",
        "print (\"Predicted label:\", labels[predicted])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8VJR2I5SqiGK"
      },
      "source": [
        "Step 3. Explain with respect to the true label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kObNH63pqiGK",
        "colab": {}
      },
      "source": [
        "exp = ig.explain(x[0], reference=ref[0], outc=predicted)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KQwN7sxFqiGL",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(8,4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(images[0])\n",
        "plt.xticks([], [])\n",
        "plt.yticks([], [])\n",
        "plt.title(\"Original image\")\n",
        "\n",
        "th = max(np.abs(np.min(exp)), np.abs(np.max(exp)))\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(np.sum(exp, axis=2), cmap=\"seismic\", vmin=-1*th, vmax=th)\n",
        "plt.xticks([], [])\n",
        "plt.yticks([], [])\n",
        "plt.title(\"Explanation\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGe_ST88s0oT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clipping\n",
        "print ('Clipping')\n",
        "print ('The two graphs below show the top and bottom clipping on the attribution distribution curve.')\n",
        "\n",
        "show_pil_image(pil_image(Visualize(\n",
        "    exp, images[0] * 255,\n",
        "    clip_above_percentile=95,\n",
        "    clip_below_percentile=30,\n",
        "    overlay=True,\n",
        "    plot_distribution=True)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNjjtNq3s0l2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Morphological cleanup\n",
        "print ('Clipping + Morphological cleanup')\n",
        "\n",
        "show_pil_image(pil_image(Visualize(\n",
        "    exp, images[0] * 255,\n",
        "    clip_above_percentile=95,\n",
        "    clip_below_percentile=30,\n",
        "    morphological_cleanup=True,\n",
        "    overlay=True)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKzgbBiLs0ju",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Outlines\n",
        "print ('Outlines')\n",
        "\n",
        "show_pil_image(pil_image(Visualize(\n",
        "    exp, images[0] * 255,\n",
        "    clip_above_percentile=95,\n",
        "    clip_below_percentile=30,\n",
        "    morphological_cleanup=True,\n",
        "    outlines=True,\n",
        "    overlay=True)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "75uaCXrYqiFH"
      },
      "source": [
        "# SHAP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gJVyKN1KqiFI"
      },
      "source": [
        "출처: https://slundberg.github.io/shap/notebooks/ImageNet%20VGG16%20Model%20with%20Keras.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P7degNSIqiFI",
        "colab": {}
      },
      "source": [
        "!pip install shap"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BMr__DddqiFJ",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.preprocessing import image\n",
        "import requests\n",
        "from skimage.segmentation import slic\n",
        "import matplotlib.pylab as plt\n",
        "import numpy as np\n",
        "import shap\n",
        "from PIL import Image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wW1cZMFHHQNq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "colors = []\n",
        "for l in np.linspace(1,0,100):\n",
        "    colors.append((245/255,39/255,87/255,l))\n",
        "for l in np.linspace(0,1,100):\n",
        "    colors.append((24/255,196/255,93/255,l))\n",
        "cm = LinearSegmentedColormap.from_list(\"shap\", colors)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "olQnCosUqiFO"
      },
      "source": [
        "#### import model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "u7rH_vwDqiFO",
        "colab": {}
      },
      "source": [
        "feature_names=['NG','OK']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ohBJDKMWqiFQ",
        "colab": {}
      },
      "source": [
        "model = tf.keras.models.load_model(model_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "i_49A4gdqiFR"
      },
      "source": [
        "### Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jmm3RN7dGhUn",
        "colab": {}
      },
      "source": [
        "sample_size = 1 # viualization할 이미지 개수, 현재 코드 상에서는 1\n",
        "class_idx = 0 # NG 이면 0, OK 이면 1\n",
        "iter = make_class_sapling(validation_fns, class_idx = class_idx, batch_size = sample_size)\n",
        "\n",
        "images, labels = iter.get_next()\n",
        "with tf.Session() as sess:\n",
        "  imgs = sess.run(images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82blYTFSGoYW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img=Image.fromarray((imgs[0]*255).astype(np.uint8))\n",
        "img_orig=imgs[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V6mofY-1qiFU",
        "colab": {}
      },
      "source": [
        "## 원래 코드인 segmentation 변환을 하면 결과가 너무 안 좋아서 image 자체를 사용해 봄\n",
        "# segment the image so we don't have to explain every pixel\n",
        "#segments_slic = slic(img, n_segments=30, compactness=30, sigma=3)\n",
        "#segments_slic = slic(img, n_segments=40000, compactness=1000, sigma=0)\n",
        "segments_slic = np.array(segimg)\n",
        "plt.imshow(segments_slic);\n",
        "plt.axis('off');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UjrRVaHGqiFV",
        "colab": {}
      },
      "source": [
        "# define a function that depends on a binary mask representing if an image region is hidden\n",
        "def mask_image(zs, segmentation, image, background=None):\n",
        "    if background is None:\n",
        "        background = image.mean((0,1))\n",
        "    out = np.zeros((zs.shape[0], image.shape[0], image.shape[1], image.shape[2]))\n",
        "    for i in range(zs.shape[0]):\n",
        "        out[i,:,:,:] = image\n",
        "        for j in range(zs.shape[1]):\n",
        "            if zs[i,j] == 0:\n",
        "                out[i][segmentation == j,:] = background\n",
        "    return out\n",
        "def f(z):\n",
        "    return model.predict((mask_image(z, segments_slic, img_orig, 255)))\n",
        "    #return model.predict(preprocess_input(mask_image(z, segments_slic, img_orig, 255)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EduU1IJSqiFZ",
        "colab": {}
      },
      "source": [
        "# use Kernel SHAP to explain the network's predictions\n",
        "explainer = shap.KernelExplainer(f, np.zeros((1,50)))\n",
        "shap_values = explainer.shap_values(np.ones((1,50)), nsamples=1000) # runs model 1000 times"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qeBKozvIqiFb",
        "colab": {}
      },
      "source": [
        "# get the top predictions from the model\n",
        "#preds = model.predict(preprocess_input(np.expand_dims(img_orig.copy(), axis=0)))\n",
        "preds = model.predict(np.expand_dims(img_orig.copy(), axis=0))\n",
        "top_preds = np.argsort(-preds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w1sM821FqiFc",
        "colab": {}
      },
      "source": [
        "def fill_segmentation(values, segmentation):\n",
        "    out = np.zeros(segmentation.shape)\n",
        "    for i in range(len(values)):\n",
        "        out[segmentation == i] = values[i]\n",
        "    return out\n",
        "\n",
        "# plot our explanations\n",
        "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(12,3))\n",
        "inds = top_preds[0]\n",
        "axes[0].imshow(img)\n",
        "axes[0].axis('off')\n",
        "max_val = np.max([np.max(np.abs(shap_values[i][:,:-1])) for i in range(len(shap_values))])\n",
        "for i in range(2):\n",
        "    m = fill_segmentation(shap_values[inds[i]][0], segments_slic)\n",
        "    axes[i+1].set_title(feature_names[inds[i]])\n",
        "    axes[i+1].imshow(img.convert('LA'), alpha=0.15)\n",
        "    im = axes[i+1].imshow(m, cmap=cm, vmin=-max_val, vmax=max_val)\n",
        "    axes[i+1].axis('off')\n",
        "cb = fig.colorbar(im, ax=axes.ravel().tolist(), label=\"SHAP value\", orientation=\"horizontal\", aspect=60)\n",
        "cb.outline.set_visible(False)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cGDBBMvdqiEl"
      },
      "source": [
        "# LIME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zMsyqfQUqiEl"
      },
      "source": [
        "출처: https://github.com/marcotcr/lime/blob/master/doc/notebooks/Tutorial%20-%20Image%20Classification%20Keras.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YVIxBrAaqiEm",
        "colab": {}
      },
      "source": [
        "!pip install lime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "z06Si8b1qiEn",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import keras\n",
        "from keras.applications import inception_v3 as inc_net\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.imagenet_utils import decode_predictions\n",
        "from skimage.io import imread\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "from skimage.segmentation import mark_boundaries\n",
        "print('Notebook run using keras:', keras.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WnH8cA9DqiEq"
      },
      "source": [
        "#### import model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fWC_XPDyqiEq",
        "colab": {}
      },
      "source": [
        "model = tf.keras.models.load_model(model_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tL05Ob9SqiEt"
      },
      "source": [
        "Let's see the top 5 prediction for some image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KyZAak1F96gO",
        "colab": {}
      },
      "source": [
        "sample_size = 1 # viualization할 이미지 개수, 현재 코드 상에서는 1\n",
        "class_idx = 0 # NG 이면 0, OK 이면 1\n",
        "iter = make_class_sapling(validation_fns, class_idx = class_idx , batch_size = sample_size)\n",
        "\n",
        "images, labels = iter.get_next()\n",
        "with tf.Session() as sess:\n",
        "  images = sess.run(images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSBCMBR493Mr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.imshow(images[0])\n",
        "preds = model.predict(images)\n",
        "print (preds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YB1TQSRjqiEu"
      },
      "source": [
        "#### Explanation\n",
        "Now let's get an explanation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8ARepTsvqiEv",
        "colab": {}
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "import os,sys\n",
        "try:\n",
        "    import lime\n",
        "except:\n",
        "    sys.path.append(os.path.join('..', '..')) # add the current directory\n",
        "    import lime\n",
        "from lime import lime_image\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5S7ocvCEbO7",
        "colab_type": "text"
      },
      "source": [
        "Parameters:\t\n",
        "- training_data – numpy 2d array\n",
        "- training_labels – labels for training data. Not required, but may be used by discretizer.\n",
        "- feature_names – list of names (strings) corresponding to the columns in the training data.\n",
        "- categorical_features – list of indices (ints) corresponding to the categorical columns. Everything else will be considered continuous. Values in these columns MUST be integers.\n",
        "- categorical_names – map from int to list of names, where categorical_names[x][y] represents the name of the yth value of column x.\n",
        "- kernel_width – kernel width for the exponential kernel.\n",
        "- None, defaults to sqrt (If) –\n",
        "- verbose – if true, print local prediction values from linear model\n",
        "- class_names – list of class names, ordered according to whatever the classifier is using. If not present, class names will be ‘0’, ‘1’, ...\n",
        "- feature_selection – feature selection method. can be ‘forward_selection’, ‘lasso_path’, ‘none’ or ‘auto’. See function ‘explain_instance_with_data’ in lime_base.py for details on what each of the options does.\n",
        "- discretize_continuous – if True, all non-categorical features will be discretized into quartiles.\n",
        "- discretizer – only matters if discretize_continuous is True. Options are ‘quartile’, ‘decile’ or ‘entropy’"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tyeTnbD6qiEx",
        "colab": {}
      },
      "source": [
        "explainer = lime_image.LimeImageExplainer(verbose=True, feature_selection='auto')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "e9RNagEJqiEy"
      },
      "source": [
        "hide_color is the color for a superpixel turned OFF. Alternatively, if it is NONE, the superpixel will be replaced by the average of its pixels. Here, we set it to 0 (in the representation used by inception model, 0 means gray)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_1WMS2CDTo0",
        "colab_type": "text"
      },
      "source": [
        "```bash\n",
        "explain_instance(image, classifier_fn, labels=(1, ), hide_color=None, top_labels=5, num_features=100000, num_samples=1000, batch_size=10, qs_kernel_size=4, distance_metric='cosine', model_regressor=None)\n",
        "```\n",
        "\n",
        "Generates explanations for a prediction.\n",
        "\n",
        "First, we generate neighborhood data by randomly perturbing features from the instance (see __data_inverse). We then learn locally weighted linear models on this neighborhood data to explain each of the classes in an interpretable way (see lime_base.py).\n",
        "\n",
        "Parameters:\t\n",
        "- data_row – 1d numpy array, corresponding to a row\n",
        "- classifier_fn – classifier prediction probability function, which takes a numpy array and outputs prediction probabilities. For ScikitClassifiers , this is classifier.predict_proba.\n",
        "- labels – iterable with labels to be explained.\n",
        "- top_labels – if not None, ignore labels and produce explanations for the K labels with highest prediction probabilities, where K is this parameter.\n",
        "- num_features – maximum number of features present in explanation\n",
        "- num_samples – size of the neighborhood to learn the linear model\n",
        "- distance_metric – the distance metric to use for weights.\n",
        "- model_regressor – sklearn regressor to use in explanation. Defaults\n",
        "- Ridge regression in LimeBase. Must have model_regressor.coef (to) –\n",
        "- 'sample_weight' as a parameter to model_regressor.fit() (and) –\n",
        "- qs_kernel_size – the size of the kernal to use for the quickshift segmentation\n",
        "\n",
        "\n",
        "Returns:\t\n",
        "An Explanation object (see explanation.py) with the corresponding explanations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S5SOKOrIqiE0",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "# Hide color is the color for a superpixel turned OFF. Alternatively, if it is NONE, the superpixel will be replaced by the average of its pixels\n",
        "#explanation = explainer.explain_instance(images[0], model.predict, top_labels=2, hide_color=0, num_features=1000, num_samples=2000)\n",
        "explanation = explainer.explain_instance(images[0], model.predict, top_labels=2, hide_color=0, num_samples=3000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vSlhyfg-qiE2"
      },
      "source": [
        "Image classifiers are a bit slow. Notice that an explanation on my Surface Book dGPU took 1min 29s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Rklod8giqiE2"
      },
      "source": [
        "#### Now let's see the explanation for the top class\n",
        "We can see the top 1 superpixels that are most positive towards the class with the rest of the image hidden"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZP0qB0v2GLjN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=5, hide_rest=True)\n",
        "plt.imshow(mark_boundaries(temp / 2 + 0.5, mask))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5n12Cb9Vpv9",
        "colab_type": "text"
      },
      "source": [
        "Or with the rest of the image present:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xlYm4El1qiE6",
        "colab": {}
      },
      "source": [
        "temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=5, hide_rest=False)\n",
        "plt.imshow(mark_boundaries(temp / 2 + 0.5, mask))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "13FdEw8nqiE8"
      },
      "source": [
        "We can also see the 'pros and cons' (pros in green, cons in red)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LIRLqSyUqiE8",
        "colab": {}
      },
      "source": [
        "temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=False, num_features=10, hide_rest=False)\n",
        "plt.imshow(mark_boundaries(temp / 2 + 0.5, mask))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gMnCiEPzqiE9"
      },
      "source": [
        "Or the pros and cons that have weight at least 0.1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7SRpMrSCqiE-",
        "colab": {}
      },
      "source": [
        "temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=False, num_features=1000, hide_rest=False, min_weight=0.1)\n",
        "plt.imshow(mark_boundaries(temp / 2 + 0.5, mask))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jTvx32tKqiE_"
      },
      "source": [
        "Let's see the explanation for the second highest prediction\n",
        "Most positive towards wombat:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xbK94qHSqiFA",
        "colab": {}
      },
      "source": [
        "temp, mask = explanation.get_image_and_mask(explanation.top_labels[1], positive_only=False, num_features=5, hide_rest=False)\n",
        "plt.imshow(mark_boundaries(temp / 2 + 0.5, mask))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3f0saRUZqiFB"
      },
      "source": [
        "Pros and cons:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mZhooCsdqiFC",
        "colab": {}
      },
      "source": [
        "temp, mask = explanation.get_image_and_mask(explanation.top_labels[1], positive_only=False, num_features=10, hide_rest=False)\n",
        "plt.imshow(mark_boundaries(temp / 2 + 0.5, mask))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HmTq06sWoOA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}